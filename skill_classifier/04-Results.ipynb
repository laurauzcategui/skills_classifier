{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef55d3b4",
   "metadata": {},
   "source": [
    "# Analysis & Results\n",
    "\n",
    "\n",
    "The following notebook is a summary of the Results after working with different Baselines and Models. \n",
    "\n",
    "Models used: \n",
    "\n",
    "#### Baselines \n",
    "\n",
    "1. **Majority Class with Feature Extraction**\n",
    "1. **Logistic Regression Classifier with Feature Extraction**\n",
    "\n",
    "#### Models \n",
    "\n",
    "1. **Ensemble Model for German using `de_core_news_lg` package with spaCy** - without cleaning the Data \n",
    "2. **Ensemble Model for German using `de_core_news_lg` package with spaCy** - with cleaning the Data by removing Punctuation, stop words and lemmatizing the tokens\n",
    "3. **BERT Transformers model for German fine-tune on `bert-base-german-cased` with our data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffb906",
   "metadata": {},
   "source": [
    "## Baselines definition\n",
    "\n",
    "It's well known that before starting to use / train any models, one should create baselines that will serve as comparisons with the models we would like to test. \n",
    "\n",
    "I have chosen to work as follows: \n",
    "\n",
    "- After pre-processing the data, to be able to work with the text data, I have worked with transformers model `bert-base-german-cased` to build a FeatureExtractor as I wanted to take advantage of the German training data used for this model. \n",
    "\n",
    "\n",
    "______________\n",
    "\n",
    "#### Side note on Feature Extractor \n",
    "\n",
    "As we want to vectorize the data to be able to use it in our models, I have frozen the weights of the model and get the hidden_states from the output of the CLS token, so that those could be used as Features for the baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50089e69",
   "metadata": {},
   "source": [
    "## Model selection \n",
    "\n",
    "Initially, I wanted to try different models and selected two different ones, described next: \n",
    "\n",
    "### 1. TextCategorizer Ensemble model from spaCy. \n",
    "\n",
    "Quote: \n",
    "> Stacked ensemble of a linear bag-of-words model and a neural network model.\n",
    "\n",
    "**Notes:** \n",
    "- Usually ensemble models perform well for classification, in this particular case it seems the updated model on the training data achieved good results that will be discussed later on\n",
    "- One of the caveats of this model is that bag-of-words models usually doesn't account for context and surroudings around the text. \n",
    "- spaCy it's specialized to build multi-lingual pipelines, this particular one has multiple sources for training such as TIGER corpus, Tiger2Dep, explosion fastText vectors. \n",
    "\n",
    "Reference: [spaCy de_core_news_lg](https://spacy.io/models/de#de_core_news_lg)\n",
    "\n",
    "### 2. Transformer models. \n",
    "\n",
    "Based on previous experience, working with text data it's challenging and luckily more recently we have seen an increase usage of Transformer Models, which internally uses Attention mechanisms for looking around surrounding text and not only the token that is being processed. \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- Model chosen was based on the fact that: \n",
    "    - It was trained on German corpus \n",
    "    - Training corpus was based on : `German Wikipedia, OpenLegalData, News Articles` which seems a big bigger and more diverse than the spaCy model.\n",
    "    \n",
    "Reference: [Deepset - German BERT article](https://www.deepset.ai/german-bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e128dcf",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ecfe4",
   "metadata": {},
   "source": [
    "## Baseline Results\n",
    "\n",
    "The following metrics were chosen for evaluation: \n",
    "\n",
    "- Accuracy, as it's one of the metrics usually utilized for evaluating Classification models. \n",
    "- For the case of the spaCy models, the metric is set to macro-f1 by default. \n",
    "\n",
    "To visualize the results, a *Confusion Matrix* was utilized. \n",
    "\n",
    "### Baseline 1. Majority Class Results\n",
    "\n",
    "As it's expected the results from this Classifier will be always set to predict the majority class no matter what the real class is. \n",
    "\n",
    "**Accuracy on Test Set**: 42.1 % \n",
    "\n",
    "**Analysis**: \n",
    "\n",
    "- This seems to be fair given we are always predicting the majority of the class, recalling from the class distribution and the proportion of the class was 42%\n",
    "\n",
    "<img src=\"./assets/distrib_per_class.png\" alt=\"Dataset Distribution per Class\" width=350 height=350/>\n",
    "\n",
    "\n",
    "### Baseline 2. Logistic Regression Classifier Results\n",
    "\n",
    "This model was trained for 3000 iterations, and it seems to have achieved really good results on the Test set. \n",
    "\n",
    "**Accuracy on Test Set**: 94 %\n",
    "\n",
    "**Analysis**: \n",
    "\n",
    "Even though Logistic Regression is a simple classifier, it's still powerful and in the case of the dataset it performed really well. \n",
    "\n",
    "We can observe in the confusion matrix below, how the accuracy for the model was above 90% for all the classes as:\n",
    "\n",
    "- 91% Correct predicted labels for the `soft` class\n",
    "- 93% Correct predicted labels for the `tech` class\n",
    "- 97% Correct predicted labels for the `none` class \n",
    "\n",
    "<img src=\"./assets/log_reg_matrix.png\" alt=\"Confusion Matrix - Logistic Regression\" width=400 height=400/>\n",
    "\n",
    "**Notes**: \n",
    "- It would be interesting to perform error analysis for the following classes where the wrong class was predicted: \n",
    "    - Predicted: Soft, True: Tech, with a 5% error. \n",
    "    - Predicted: Tech, True: Soft, with a 7% error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20199465",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d09d78c",
   "metadata": {},
   "source": [
    "## Models Results\n",
    "\n",
    "### 1. Ensemble Model from spaCy -  No cleaning\n",
    "\n",
    "This was a really interesting experiment as I am not 100% familiar with the library but without doubt powerful and with lots of capabilities for building NLP pipelines for downstream tasks such as: PoS, NER , Lemmatizers among others.\n",
    "\n",
    "**Macro-F1 on Test Set**:  93.65 %\n",
    "\n",
    "<img src=\"./assets/c_matrix_raw.png\" alt=\"Confusion Matrix - spacy raw data\" width=400 height=400/>\n",
    "\n",
    "**Analysis**: \n",
    "\n",
    "It seems the model performed really well in terms of predicting the `tech` class. Based on the requirements for this application, one could select this model if for example the customer is looking to obtain candidates, job reqs focused in Tecnology. \n",
    "\n",
    "\n",
    "### 2. Ensemble Model from spaCy - Cleaning was performed \n",
    "\n",
    "This experiment differs from the first model as the Punctuation, StopWords were removed and posteriorly the text was lemmatized. \n",
    "\n",
    "**Macro-F1 on Test Set**: 89.29 %\n",
    "\n",
    "Let's observed the Confusion Matrix below. \n",
    "\n",
    "<img src=\"./assets/c_matrix_clean.png\" alt=\"Confusion Matrix - Cleaned data\" width=400 height=400/>\n",
    "\n",
    "**Analysis**:\n",
    "\n",
    "Hypothesis was based on that cleaning the data by removing punctuation and stopwords and further addition of lemmatizer might help, but It seems it didn't. I would be curious to do another round with a native speaker. Perhaps context was lost after removing certain stopwords and the model couldn't generalize well.\n",
    "\n",
    "### 3. German BERT Transformer Model\n",
    "\n",
    "After fine-tuning in the data provided for training and validation sets, the best model seems to be this one, with an Accuracy of around 95% \n",
    "\n",
    "**Accuracy on Test Set**: 95% \n",
    "\n",
    "\n",
    "**Analysis**: \n",
    "\n",
    "Checking once more the confusion matrix for the BERT model, we can observe how the predictions were better. One of the reasons this model might be outperforming the others apart from the fact that is using Transformers, is that the metrics were calculated taking in account the class imbalance from the `none` class. \n",
    "\n",
    "\n",
    "<img src=\"./assets/c_matrix_bert.png\" alt=\"Confusion Matrix - BERT\" width=400 height=400/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13548807",
   "metadata": {},
   "source": [
    "# Lessons learnt and further improvements\n",
    "\n",
    "This challenge was very rewarding and fun!!\n",
    "\n",
    "### Lessons learnt\n",
    "\n",
    "- Language Barrier, though I don't know German but just very basics I was able to pull through with the classifier successfully.\n",
    "- Cleaning data took more time than expected. As I didn't noticed I was deleting the duplicated incorrectly I had to re-run the experiments twice. \n",
    "- Library learning curve, spaCy is a powerful and well-built library, I wish I have more time to tweak parameters or train a Categorizer from Scratch. \n",
    "- Domain knowledge, I think I might have perform a better task if I would have asked if the `none` class was relevant or not for the classification task. \n",
    "---------------\n",
    "\n",
    "### Further Improvements to the Model and the Work in General\n",
    "\n",
    "- Perform Error Analysis to check what is happening around the missclassifications of the tech label being confused with the soft labels.\n",
    "- Perform downsampling to the `none` class if it's truly relevant \n",
    "- With the hypothesis that `none` class is not important, I think a model could be build for classifying the text in skills (soft or tech) \n",
    "- I have the intuition of if given time and annotated data, training a NER on the data could have helped by identifying certain \"tools\", \"skills\", \"software\" and the classifier could have been improved by this.\n",
    "- I think the Transformer models performed really well given it didn't have a lot of data to train with, but have its downsides such as Size and Carbon footprint. It would be interesting to build an hypothesis around distillation models which are light in weight and training time in terms of resources like cost and memory (GPU). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a9039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
